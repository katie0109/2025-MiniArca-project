import hashlib
import json
import re
import requests
import time

# Ollama API ì„¤ì •
OLLAMA_URL = "http://localhost:11434/api/generate"
OLLAMA_MODEL = "llama3"  # ì„¤ì¹˜í•œ ëª¨ë¸ ì´ë¦„

response_cache = {}

# Ollama í˜¸ì¶œ í•¨ìˆ˜
def call_ollama_model(full_prompt):
    response = requests.post(
        OLLAMA_URL,
        json={
            "model": OLLAMA_MODEL,
            "prompt": full_prompt,
            "stream": False
        }
    )
    response.raise_for_status()
    response_json = response.json()
    return response_json["response"]

# JSON íŒŒì‹±
def extract_json(text):
    json_match = re.search(r"\{.*\}", text, re.DOTALL)
    if json_match:
        return json_match.group(0)
    else:
        raise ValueError("JSON ì‘ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# ê³µí†µ ìºì‹œ
def cache_response(text, key_suffix, data):
    text_hash = hashlib.md5((text + key_suffix).encode('utf-8')).hexdigest()
    response_cache[text_hash] = data
    return data

def get_cached_response(text, key_suffix):
    text_hash = hashlib.md5((text + key_suffix).encode('utf-8')).hexdigest()
    return response_cache.get(text_hash)

# ê°ì • ë¶„ì„
def emotion_anal(text):
    cached_response = get_cached_response(text, "_emotion")
    if cached_response:
        return cached_response
    
    start_time = time.time()
    instructions = """
ì£¼ì–´ì§„ í•œêµ­ì–´ í…ìŠ¤íŠ¸ì—ì„œ ì „ë°˜ì ì¸ ë‚´ìš©ì„ ê¹Šì´ ìˆê²Œ ì´í•´í•˜ì—¬ ê°ì •ì„ ë¶„ì„í•˜ì„¸ìš”.
ê°€ì¥ ë†’ì€ ê°•ë„ë¥¼ ê°€ì§€ëŠ” ê°ì •ì„ ì£¼ìš” ê°ì •ìœ¼ë¡œ ì„ íƒí•˜ì„¸ìš”.
ì •í™•íˆ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{
    "ì£¼ìš” ê°ì •": "ê°ì • ì´ë¦„",
    "ê°ì • ê°•ë„": ìˆ«ì (0~100),
    "ì„¸ë¶€ ê°ì •": [
        {"ê°ì •": "ê°ì • ì´ë¦„", "ê°•ë„": ìˆ«ì (0~100)},
        {"ê°ì •": "ê°ì • ì´ë¦„", "ê°•ë„": ìˆ«ì (0~100)},
        {"ê°ì •": "ê°ì • ì´ë¦„", "ê°•ë„": ìˆ«ì (0~100)}
    ]
}
"""
    full_prompt = f"{instructions}\ní…ìŠ¤íŠ¸: {text}\nì‘ë‹µ:"
    response = call_ollama_model(full_prompt)
    elapsed_time = time.time() - start_time
    print(f"ğŸ•’ emotion_anal ì‹¤í–‰ ì‹œê°„: {elapsed_time:.2f}ì´ˆ")
    return cache_response(text, "_emotion", json.loads(extract_json(response)))

# ì¥ì†Œ ì¶”ì¶œ
def extract_places(text):
    cached_response = get_cached_response(text, "_places")
    if cached_response:
        return cached_response
    
    start_time = time.time()
    instructions = """
ì£¼ì–´ì§„ í•œêµ­ì–´ í…ìŠ¤íŠ¸ì—ì„œ ê°€ì¥ í•µì‹¬ì ì¸ ì¥ì†Œ í•˜ë‚˜ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.
ê³ ìœ ëª…ì‚¬ëŠ” ë³´í†µëª…ì‚¬ë¡œ ë°”ê¿”ì£¼ì„¸ìš”. (ì˜ˆ: 'ì„œìš¸ì—­' â†’ 'ê¸°ì°¨ì—­')
ì •í™•íˆ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{
    "ì¥ì†Œ": "ì¥ì†Œ ì´ë¦„"
}
ì¤‘ìš”: ì‘ë‹µì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ê³ , ìœ„ì— ì œì‹œëœ JSON í˜•ì‹ì„ ì •í™•íˆ ë”°ë¼ì£¼ì„¸ìš”.
"""
    full_prompt = f"{instructions}\ní…ìŠ¤íŠ¸: {text}\nì‘ë‹µ:"
    response = call_ollama_model(full_prompt)
    elapsed_time = time.time() - start_time
    print(f"ğŸ•’ extract_places ì‹¤í–‰ ì‹œê°„: {elapsed_time:.2f}ì´ˆ")
    return cache_response(text, "_places", json.loads(extract_json(response)))

# ì‚¬ë¬¼ í‚¤ì›Œë“œ ì¶”ì¶œ
def extract_object_keywords(text, excluded_keywords=None):
    cached_response = get_cached_response(text, "_object_keywords")
    if cached_response:
        return cached_response
    
    start_time = time.time()
    instructions = """
ì£¼ì–´ì§„ í•œêµ­ì–´ í…ìŠ¤íŠ¸ì—ì„œ ì¥ì†Œì™€ ê´€ë ¨ ì—†ëŠ” ì£¼ìš” ëª…ì‚¬ 5ê°œë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.
ì •í™•íˆ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{
    "ì‚¬ë¬¼ í‚¤ì›Œë“œ": ["ëª…ì‚¬1", "ëª…ì‚¬2", "ëª…ì‚¬3", "ëª…ì‚¬4", "ëª…ì‚¬5"]
}
ì¤‘ìš”: ì‘ë‹µì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ê³ , ìœ„ì— ì œì‹œëœ JSON í˜•ì‹ì„ ì •í™•íˆ ë”°ë¼ì£¼ì„¸ìš”.
ë‹¨, ì¥ì†Œì™€ ê´€ë ¨ëœ í‚¤ì›Œë“œëŠ” ì œì™¸í•˜ê³ , ì‚¬ë¬¼ í‚¤ì›Œë“œë§Œ ì¶”ì¶œí•˜ì„¸ìš”.
"""
    if excluded_keywords:
        instructions += f"\në‹¨, ë‹¤ìŒ í‚¤ì›Œë“œëŠ” ì œì™¸í•˜ì„¸ìš”: {', '.join(excluded_keywords)}."

    full_prompt = f"{instructions}\ní…ìŠ¤íŠ¸: {text}\nì‘ë‹µ:"
    response = call_ollama_model(full_prompt)
    elapsed_time = time.time() - start_time
    print(f"ğŸ•’ extract_places ì‹¤í–‰ ì‹œê°„: {elapsed_time:.2f}ì´ˆ")
    return cache_response(text, "_object_keywords", json.loads(extract_json(response)))

# 60ì ì´ë‚´ ìš”ì•½
def summarize_text(text):
    cached_response = get_cached_response(text, "_summary")
    if cached_response:
        return cached_response
    
    start_time = time.time()
    instructions = """
ë‹¤ìŒ í•œêµ­ì–´ í…ìŠ¤íŠ¸ë¥¼ ìµœëŒ€í•œ í•µì‹¬ ë‚´ìš©ë§Œ ë°˜ì˜í•˜ì—¬ ìš”ì•½í•˜ì„¸ìš”.
ìš”ì•½ ë¬¸ì¥ì€ ë„ì–´ì“°ê¸° í¬í•¨ 60ìë¥¼ ë„˜ì§€ ì•Šì•„ì•¼ í•˜ë©°, ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ ë¬¸ì¥ìœ¼ë¡œ ë§Œë“¤ì–´ì£¼ì„¸ìš”.
ì •í™•íˆ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{
    "ìš”ì•½": "ìš”ì•½ëœ ë¬¸ì¥"
}
ì¤‘ìš”: ì‘ë‹µì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ê³ , ìœ„ì— ì œì‹œëœ JSON í˜•ì‹ì„ ì •í™•íˆ ë”°ë¼ì£¼ì„¸ìš”.
"""
    full_prompt = f"{instructions}\ní…ìŠ¤íŠ¸: {text}\nì‘ë‹µ:"
    response = call_ollama_model(full_prompt)
    elapsed_time = time.time() - start_time
    print(f"ğŸ•’ extract_places ì‹¤í–‰ ì‹œê°„: {elapsed_time:.2f}ì´ˆ")
    return cache_response(text, "_summary", json.loads(extract_json(response)))

# í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    sample_text = "ì˜¤ëŠ˜ì€ í•œê°• ê³µì›ì— ë‹¤ë…€ì™”ë‹¤. íë¦° ë‚ ì”¨ì˜€ì§€ë§Œ ë°”ëŒì´ ì‹œì›í•´ì„œ ì‚°ì±…í•˜ê¸° ì¢‹ì•˜ë‹¤. ê°•ì„ ë”°ë¼ ê±·ë‹¤ ë³´ë‹ˆ ë§ˆìŒì´ í•œê²° ê°€ë²¼ì›Œì§€ëŠ” ëŠë‚Œì´ì—ˆë‹¤. ë²¤ì¹˜ì— ì•‰ì•„ ì»¤í”¼ë¥¼ ë§ˆì‹œë©° í•œì°¸ì„ ê°•ë¬¼ íë¦„ë§Œ ë°”ë¼ë´¤ë‹¤. ë°”ìœ ì¼ìƒ ì† ì ì‹œ ìˆ¨ì„ ê³ ë¥¼ ìˆ˜ ìˆì–´ ê³ ë§ˆìš´ ì‹œê°„ì´ì—ˆë‹¤."
    result = emotion_anal(sample_text)
    print(json.dumps(result, ensure_ascii=False, indent=2))






